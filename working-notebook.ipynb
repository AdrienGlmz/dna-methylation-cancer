{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Cancer with DNA Methylation\n",
    "\n",
    "This notebook is a walk-through of our project submitted to the 2020 Hack for Social Good\n",
    " _Build Hackathon.\n",
    "\n",
    "This notebook is organized the following way:\n",
    " 1. Data preparation\n",
    " 2. Preprocessing\n",
    " 3. Model Training and Evaluation\n",
    " 4. Model Deployment\n",
    "\n",
    "It uses data from the [TCGA Project](https://www.genome.gov/Funded-Programs-Projects/Cancer-Genome-Atlas)\n",
    "to create a BigQuery table. This table is then pre-processed to create our final training set.\n",
    "We then go through model training and model deployment.\n",
    "At the end of the notebook, you'll get a trained and deployed model on AI Prediction.\n",
    "\n",
    "This other [repo](https://github.com/jeremy0dell/build-hackathon) uses this deployed\n",
    "model to publish a React webapp on GCP and provide a user interface to interact with\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment setup\n",
    "This notebook has been tested with Python 3.8.5. If not done already, create the `DNA_Methylation`\n",
    "virtual environment using conda and the `environment.yml` file.\n",
    "\n",
    "### Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import ML libraries\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Import local libraries\n",
    "from feature_engineering.get_data import read_from_gcs\n",
    "from feature_engineering.preprocessing import preprocessing\n",
    "from configs import configs\n",
    "\n",
    "# Import GCP libraries\n",
    "from google.cloud import bigquery, storage\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We tested this notebook with the following version of those packages\n",
    " - Pandas: 1.1.4\n",
    " - Scikit-learn: 0.23.2\n",
    " - XGBoost: 1.2.0\n",
    "\n",
    "Check that your environment uses those versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas version used: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version used: {sklearn.__version__}\")\n",
    "print(f\"XGBoost version used: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GCP Environment\n",
    "We heavily uses GCP services on this Notebook.\n",
    "\n",
    "Retrieve credentials to use Google Cloud's Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcp-nyc\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = configs.PROJECT_ID\n",
    "DATASET = configs.DATASET\n",
    "BETAS_TABLE_NAME = configs.BETAS_TABLE_NAME\n",
    "CPG_SITE_TABLE_NAME = configs.CPG_SITE_TABLE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "In this first part, we will create two bigquery tables that will used to create the\n",
    "final train and test datasets.\n",
    "\n",
    "The SQL script are located in the `sql_queries` folder.\n",
    "\n",
    "The first BigQuery table is called `tcga_betas` is a BigQuery partitioned and clustered table\n",
    "that will hold all betas values for all TCGA patients. It will be a table in a long format:\n",
    "one row per betas observation.\n",
    "\n",
    "The second BigQuery table is called `columns_to_keep` is a BigQuery table that will hold the\n",
    "list of CpG site that we will keep in the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First table - `tcga_betas` table: a simple SQL script\n",
    "\n",
    "This first table will be created using a simple SQL query. The query is located in\n",
    "`SQL_queries/tcga_betas.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This table will have the following format:\n",
    "\n",
    "| case_barcode | beta_value         | CpG_probe_id | aliquot_barcode              | sample_id | sample_status | row_number |\n",
    "| ------------ | ------------------ | ------------ | ---------------------------- | --------- | ------------- | ---------- |\n",
    "| TCGA-HC-A6AL | 0.904757283874934  | cg08616243   | TCGA-HC-A6AL-01A-11D-A30F-05 | 01        | tumor         | 8303       |\n",
    "| TCGA-HC-A6AL | 0.361544714195919  | cg22309923   | TCGA-HC-A6AL-01A-11D-A30F-05 | 01        | tumor         | 8303       |\n",
    "| TCGA-HC-A6AL | 0.0234784367599006 | cg26917673   | TCGA-HC-A6AL-01A-11D-A30F-05 | 01        | tumor         | 8303       |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE dna_cancer_prediction.tcga_betas\n",
      " PARTITION BY RANGE_BUCKET(row_number, GENERATE_ARRAY(1, 11000, 100))\n",
      " CLUSTER BY row_number\n",
      " AS\n",
      " \n",
      " WITH\n",
      "   brca_betas AS (\n",
      "   SELECT\n",
      "     MIM.CpG_probe_id,\n",
      "     dna.case_barcode,\n",
      "     dna.aliquot_barcode,\n",
      "     dna.probe_id,\n",
      "     dna.beta_value,\n",
      "     gc.Name\n",
      "   FROM\n",
      "     `isb-cgc.platform_reference.methylation_annotation` gc\n",
      "   JOIN\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` dna\n",
      "   ON\n",
      "     gc.Name = dna.probe_id\n",
      "   JOIN\n",
      "     `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` MIM\n",
      "   ON\n",
      "     MIM.CpG_probe_id = gc.Name),\n",
      " \n",
      "   participants_id AS (\n",
      "   SELECT\n",
      "     DISTINCT case_barcode\n",
      "   FROM\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation`),\n",
      " \n",
      "   participants_id_numbered AS (\n",
      "   SELECT\n",
      "     case_barcode,\n",
      "     ROW_NUMBER() OVER (order by case_barcode) as row_number\n",
      "   FROM\n",
      "     participants_id)\n",
      " \n",
      " SELECT\n",
      "   A.case_barcode,\n",
      "   A.beta_value,\n",
      "   A.CpG_probe_id,\n",
      "   A.aliquot_barcode,\n",
      "   SUBSTR(A.aliquot_barcode, 14, 2) AS sample_id,\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('10', '11', '12', '13', '14', '15', '16', '17', '18', '19'), \"normal\",\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('01', '02', '03', '04',  '05', '06', '07', '08', '09'), 'tumor', \"control\")) AS sample_status,\n",
      "   B.row_number\n",
      " FROM\n",
      "   brca_betas A\n",
      " LEFT JOIN\n",
      "   participants_id_numbered B\n",
      " ON A.case_barcode = B.case_barcode\n"
     ]
    }
   ],
   "source": [
    "SQL_query_path = 'sql_queries/tcga_betas.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TABLE_NAME__', BETAS_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uncomment the line below to print the SQL query"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(sql_query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TCGA has published in BigQuery an open dataset called `isb-cgc` (documentation\n",
    "[here](https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/data/TCGA_top.html)).\n",
    "We are using three tables from the `isb-cgc` as source tables:\n",
    "  1. `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` is the main table where the DNA Methylation values\n",
    "  are found.\n",
    "  2. `isb-cgc.platform_reference.methylation_annotation` is used to retrieve the CpG site names\n",
    "  3. `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` is used to retrieve the CpG site ids.\n",
    "\n",
    "We are creating our raw table in two parts:\n",
    " 1. creating the `brca_betas` table that contains a join of those three tables mentioned above\n",
    " 2. selecting columns from this `brca_betas` table and creating the `sample_status` field that will\n",
    " be used as prediction label.\n",
    "\n",
    "To build the `sample_status`, we are using the `aliquot_barcode` field from TCGA data and more\n",
    "specifically the sample part of the barcode. According to the\n",
    "[documentation](https://docs.gdc.cancer.gov/Encyclopedia/pages/TCGA_Barcode/), the sample part\n",
    "gives information about the label cancerous status:\n",
    " - if the sample is between 01 and 09, the tissue is cancerous\n",
    " - else if between 10 and 19, the tissue is normal\n",
    " - else it is a control sample.\n",
    "\n",
    "We can execute the job to create the table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Second table - `columns_to_keep`: using dataproc\n",
    "\n",
    "The second table will hold the list of CpG sites that will be included in the training dataset.\n",
    "\n",
    "Indeed, we want to keep only the 5,000 best CpG site among the 500,000 that will be able to\n",
    "separate cancerous vs non-cancerous observations.\n",
    "\n",
    "This table will be called `cpg_site_list` created in the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This table will only have a single column and 5,000 rows\n",
    "\n",
    "| CpG_probe_id |\n",
    "| ------------ |\n",
    "| cg21270593   |\n",
    "| cg10107671   |\n",
    "| cg11768416   |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SQL_query_path = 'sql_queries/cpg_site_list.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TCGA_BETAS_TABLE__', BETAS_TABLE_NAME)\n",
    "sql_query = sql_query.replace('__CPG_SITE_TABLE_NAME__', CPG_SITE_TABLE_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uncomment the line below to print the SQL query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(sql_query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are selecting the 5,000 CpG sites that maximize the variance in-between\n",
    "groups (i.e. normal and tumor patient). For this, we are applying the following\n",
    "steps in order:\n",
    "  1. computing the count of values per CpG sites\n",
    "  2. filtering out CpG sites which are mesured in less than 90% of patients\n",
    "  3. computing the average beta value per CpG sites\n",
    "  4. computing the average bet value per CpG sites and per label (normal /tumor)\n",
    "  5. computing the variance in-between groups\n",
    "  6. selecting the best 5,000 sites"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [81fa880ca37c4901bc736ce3e941c5ea] submitted.\n",
      "Waiting for job output...\n",
      "build_hackathon_dnanyc_tfx_pipeline\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.util.log: Logging initialized @2728ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: Started @2816ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:20:36 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at build-hackathon-nyc-cluster-m/10.0.0.39:8032\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at build-hackathon-nyc-cluster-m/10.0.0.39:10200\n",
      "20/10/20 14:20:39 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1603203106846_0002\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[aliquot_barcode], filters=[]\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[aliquot_barcode], filter=''\n",
      "20/10/20 14:20:50 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGRFamNsRlVxTHJXNBoCaXIaAmpk\n",
      "20/10/20 14:25:00 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'CpG_probe_id#2 = CpG_probe_id#2'. Perhaps you need to use aliases.\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id,sample_status], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id, sample_status], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEtCZlRZTWF4c0I3VxoCaXIaAmpk\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[CpG_probe_id], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[CpG_probe_id], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGxYLTZ2RWlsaEc4YhoCaXIaAmpk\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id], filters=[]\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id], filter=''\n",
      "20/10/20 14:25:07 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEwwSkt5V2hqQi1lcBoCaXIaAmpk\n",
      "20/10/20 14:35:11 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=dna_cancer_prediction, projectId=gcp-nyc, tableId=cpg_site_list}}. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to gcp-nyc.dna_cancer_prediction.cpg_site_list. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.deploy.yarn.Client: Failed to contact YARN for application application_1603203106846_0002.\n",
      "java.io.InterruptedIOException: Call interrupted\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1504)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1456)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1366)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:232)\n",
      "\tat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy16.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:496)\n",
      "\tat org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:301)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1060)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:109)\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend: Yarn application has already exited with state FAILED!\n",
      "Job [81fa880ca37c4901bc736ce3e941c5ea] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/\n",
      "driverOutputResourceUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/driveroutput\n",
      "jobUuid: 96de5e5c-9a94-3efb-98d4-15dc226da94f\n",
      "placement:\n",
      "  clusterName: build-hackathon-nyc-cluster\n",
      "  clusterUuid: 11f36b0a-7180-44e2-8f22-a219c38293fb\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - -gcs_bucket\n",
      "  - build_hackathon_dnanyc_tfx_pipeline\n",
      "  jarFileUris:\n",
      "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
      "  mainPythonFileUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/staging/create_cpg_sites_list.py\n",
      "reference:\n",
      "  jobId: 81fa880ca37c4901bc736ce3e941c5ea\n",
      "  projectId: gcp-nyc\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2020-10-20T14:35:23.713Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2020-10-20T14:20:31.861Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2020-10-20T14:20:31.926Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2020-10-20T14:20:32.149Z'\n",
      "yarnApplications:\n",
      "- name: my_app\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://build-hackathon-nyc-cluster-m:8088/proxy/application_1603203106846_0002/\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating training dataset\n",
    "\n",
    "We will now use those training tables to create our training dataset. The training dataset will be\n",
    "saved into a GCS bucket defined in the `configs/configs.py` file.\n",
    "\n",
    "For this, we are generating programmatically the SQL queries that filters\n",
    "out measurements that are not part of the CpG site list.\n",
    "\n",
    "We then create the final dataset in the `pivot_data` function using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = configs.RAW_DATASET_PATH\n",
    "DATASET_NAME = configs.RAW_DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def configure_gcs(gcp_project_id=PROJECT_ID):\n",
    "    current_gcs_client = storage.Client(project=gcp_project_id)\n",
    "    return current_gcs_client\n",
    "\n",
    "def save_to_gcs(current_df, gcs_path, file_name):\n",
    "    current_df.to_csv(gcs_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_bigquery(gcp_project_id, dataset, table, list_of_columns, start_index, end_index):\n",
    "    formatted_columns = \"', '\".join(list_of_columns)\n",
    "    query = f\"\"\"\n",
    "    SELECT beta_value, CpG_probe_id, aliquot_barcode, sample_status\n",
    "    from `{dataset}.{table}`\n",
    "    where CpG_probe_id in ('{formatted_columns}') and row_number >= {start_index} and row_number <= {end_index}\n",
    "    \"\"\"\n",
    "    betas_df = pd.read_gbq(query, project_id=gcp_project_id)\n",
    "    return betas_df\n",
    "\n",
    "def download_columns_to_keep(gcp_project_id, dataset, table):\n",
    "    query = f\"\"\"\n",
    "    SELECT CpG_probe_id\n",
    "    from `{dataset}.{table}`\n",
    "    \"\"\"\n",
    "    cpg_df = pd.read_gbq(query, project_id=gcp_project_id)\n",
    "    return cpg_df['CpG_probe_id'].values\n",
    "\n",
    "def merge_and_pivot_by_slices(gcp_project_id, dataset, table, cpg_list, start_index, end_index):\n",
    "    df_betas = download_from_bigquery(gcp_project_id, dataset, table, cpg_list, start_index, end_index)\n",
    "    df_p = df_betas.pivot(index=\"aliquot_barcode\", columns='CpG_probe_id', values='beta_value').reset_index()\n",
    "    df_labels = df_betas[['aliquot_barcode', 'sample_status']].drop_duplicates()\n",
    "    df_final = df_p.merge(df_labels,\n",
    "                          how='left', on='aliquot_barcode')\n",
    "    df_final = df_final.set_index('aliquot_barcode')\n",
    "    # Reorder columns\n",
    "    df_final = df_final[columns]\n",
    "    return df_final\n",
    "\n",
    "def pivot_data(cpg_project_id, dataset, betas_table, cpg_site_list_table, max_index=11000, steps=1000):\n",
    "    cpg_list = download_columns_to_keep(cpg_project_id, dataset, cpg_site_list_table)\n",
    "    df_final = merge_and_pivot_by_slices(cpg_project_id, dataset, betas_table, cpg_list, 1, steps)\n",
    "    start_index = steps + 1\n",
    "    for i in range(start_index, max_index, steps):\n",
    "        print(f\"Processing index {i} to {i + steps}\")\n",
    "        new_df = merge_and_pivot_by_slices(cpg_project_id, dataset, betas_table, columns, i, i + steps)\n",
    "        df_final = df_final.append(new_df)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gcs_client = configure_gcs(PROJECT_ID)\n",
    "columns = download_columns_to_keep(PROJECT_ID, DATASET, 'cpg_site_list')\n",
    "df = pivot_data(PROJECT_ID, DATASET, 'tcga_betas', 'cpg_site_list')\n",
    "save_to_gcs(df, DATASET_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Training\n",
    "\n",
    "Using the saved dataset in Google Cloud Storage, we will now preprocess the dataset\n",
    "to create a train and test split, train our model on the training set and output\n",
    "and evaluation metric on the evaluation set.\n",
    "\n",
    "We chose to an XGBoost model as part of this project. It's a model that first gives\n",
    "a very good accuracy on complex problems. The model is also easy to interpret\n",
    "as it is a tree based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read data from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob training_data/tcga-binary.csv downloaded to training_data.csv.\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = configs.GCS_BUCKET\n",
    "RAW_DATASET_PATH = configs.RAW_DATASET_PATH\n",
    "LABELS = configs.RAW_LABEL_NAME\n",
    "INDEX = configs.RAW_INDEX_NAME\n",
    "betas, labels, cpg_sites, index = read_from_gcs(BUCKET_NAME, RAW_DATASET_PATH, INDEX, LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Map classes to a binary integer classification. 1 will represent a tumorous observation and 0 will represent a normal\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map_labels_to_classes = {\n",
    "  'tumor': 1,\n",
    "  'normal': 0\n",
    "}\n",
    "labels = [map_labels_to_classes[elt] for elt in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apply the final preprocessing steps before starting training. In order, we will:\n",
    " 1. Drop columns that have more than 10% of values missing\n",
    " 2. Drop rows that have more than 10% of values missing\n",
    " 3. Fill remaining NA values using a `KNNImputer`\n",
    " 4. Split into a train / test dataset using 70% of data for train and 30% for the test\n",
    " 5. Standardize the train and test sets (bringing the mean and std to 0 and 1 by columns)\n",
    " 6. Balance observation in the train dataset. To get best results, we need to add\n",
    " negative (normal) observations to our dataset to get to at least 30% negative rows.\n",
    " This is done by SMOTE (*Synthetic Minority Oversample Technique*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Drop Columns and Rows ===\n",
      "Dropping 0 because of missing labels\n",
      "New Shape = (12298, 5000)\n",
      "Dropping columns which have more than 10% of values missing\n",
      "0 columns will be dropped.\n",
      "betas: New shape is (12298, 5000)\n",
      "cpg_sites: New shape is (5000,)\n",
      "\n",
      "Dropping rows which have more than 10% of values missing\n",
      "We will drop 0 rows\n",
      "betas: New shape is (12298, 5000)\n",
      "labels: New shape is (12298,)\n",
      "\n",
      "=== Fill remaining NAs ===\n",
      "Filling remaining NA values using a KNNImputer\n",
      "38743 NA were filled, i.e. approximately 3.15 per rows\n",
      "\n",
      "=== Train / Test Split ===\n",
      "Splitting dataset into train and test\n",
      "Train = 70 %\n",
      "Test = 30 %\n",
      "\n",
      "=== Standardize dataset ===\n",
      "The average of column mean on train is 0.00\n",
      "The average of column mean on test is 0.01\n",
      "\n",
      "=== Balance dataset with oversample ===\n",
      "[(0, 786), (1, 7822)]\n",
      "The resampling_strategy gives the following repartition {0: 2346, 1: 7822}\n",
      "1560 rows were added in the training data\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, labels, cpg_sites = preprocessing(betas, labels, cpg_sites, index, smote=True,\n",
    "                                                                   fill_na_strategy='knn', sampling_strategy=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can verify that we now have at least 30% of negative observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_train))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(bargroupgap=0.2,\n",
    "                  title=\"Histogram of training labels after re-balancing the dataset\",\n",
    "                  xaxis_title=\"Cancerous or normal observations\",\n",
    "                  yaxis_title=\"Count of observations\")\n",
    "fig.update_layout(\n",
    "    margin=dict(\n",
    "        l=40,\n",
    "        r=40,\n",
    "        b=40,\n",
    "        t=40,\n",
    "        pad=4\n",
    "    ),\n",
    "    title={\n",
    "        'x':0.5})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have chosen XGBoost, we need to find the hyper-parameters that will provide the best accuracy on\n",
    "our dataset.\n",
    "To do this, we are defining a Stratified-KFold cross-validation. Stratified means that we are keeping the same\n",
    "proportion of positive and negative observations in each validation fold.\n",
    "Here, we choose to create 4 folds. This means that we will do four fits: each time, training on 3/4 of the training\n",
    "data while testing on the remaining third.\n",
    "After all these fits, we average the F1 score we got and take the hyper-parameter combination that gives the best\n",
    "average F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's explain the XGBoost parameters we are using:\n",
    " - `objective=binary:logistic`: this is a binary classification problem, and we want to output the probability that\n",
    "   an observation is cancerous\n",
    " - `colsample_bytree=0.8`: we are considering only 80% of features to build each tree. This prevents over-fitting\n",
    " - `learning_rate=0.1`: prevents over-fitting by shrinking the feature weights\n",
    " - `subsample=0.8`: we are considering only 80% of the observations to build each tree. This prevents over-fitting.\n",
    " - `nthread=4`: uses 4 different thread to train the model. Enables parallel processing.\n",
    "\n",
    "In our parameter grid, we are testing the following parameters:\n",
    " - `max_depth`: the maximum depth of each tree in the model. The higher the parameter, the higher the variance\n",
    " - `min_child_weight`: the minimum sum of hessian weight needed to create a child node and partition further the tree.\n",
    "   The higher the parameter, the lower the variance.\n",
    " - `gamma`: the minimum loss reduction required to make a further partition on a leaf node. The higher the parameter,\n",
    "   the lower the variance.\n",
    " - `alpha`: L1 regularisation term on weights. The higher the parameter, the lower the variance.\n",
    " - `n_estimator`: number of trees in the model. The higher the parameter, the higher the variance.\n",
    "\n",
    "See [here](https://xgboost.readthedocs.io/en/latest/parameter.html) for the complete XGBoost documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the KFolds\n",
    "kf = StratifiedKFold(n_splits=4)\n",
    "# Define the parameter grid to test\n",
    "param_test1 = {\n",
    "    'max_depth': range(3,10,2),\n",
    "    'min_child_weight': range(1,6,2),\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'alpha':[6,8,10,12],\n",
    "    'n_estimators': [1e2, 1e3, 1e4]\n",
    "}\n",
    "estimator = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                              colsample_bytree=0.8,\n",
    "                              learning_rate=0.1,\n",
    "                              subsample=0.8,\n",
    "                              nthread=4)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=param_test1,\n",
    "                           scoring='f1_weighted',n_jobs=4, cv=5)\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's retrieve the best parameters from the GridSearch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_min_child_weight = best_params['min_child_weight']\n",
    "best_gamma = best_params['gamma']\n",
    "best_alpha = best_params['alpha']\n",
    "best_n_estimators = best_params['n_estimators']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can fit the model with the optimized hyper-parameters on the entire dataset.\n",
    "That will be our final model for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                           colsample_bytree=0.8,\n",
    "                           learning_rate=0.1,\n",
    "                           subsample=0.8,\n",
    "                           nthread=4,\n",
    "                           max_depth=10,\n",
    "                           gamma=0.1,\n",
    "                           alpha=6,\n",
    "                           n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's now analyze the model performance\n",
    "\n",
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the f1 score as a rough proxy for model performance. The F1 is the\n",
    "harmonic mean between precision and recall.\n",
    "\n",
    "$$ \\text{F1 Score} = 2 \\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "First, on the training set and then on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_predicted_labels = model.predict(X_train)\n",
    "f1_training = f1_score(training_predicted_labels, y_train)\n",
    "print(f\"The train accuracy is {f1_training:.3f}\")\n",
    "\n",
    "# Testing\n",
    "testing_predicted_labels = model.predict(X_test)\n",
    "f1_testing = f1_score(testing_predicted_labels, y_test)\n",
    "print(f\"The test accuracy is {f1_testing:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "To go one level deeper, we can output the confusion matrix on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the *training* set\")\n",
    "plot_confusion_matrix(model, X_train, y_train, ax=ax, values_format='.0f', normalize=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the test set\")\n",
    "plot_confusion_matrix(model, X_test, y_test, ax=ax, values_format='.0f', normalize=None)\n",
    "f.tight_layout()\n",
    "f.savefig(\"confusion_matrix.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training and deploying models on GCP\n",
    "\n",
    "We have trained our XGBoost model locally on this notebook. But, in a production example\n",
    "we would like to better optimize the hyper-parameter by giving more options. We will do\n",
    "this by using the AI Platform Training service.\n",
    "Once this is done, we also need to deploy our model into production. That will be done\n",
    "by the AI Platform Prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first save to GCS the train and test datasets we used earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_path = configs.TRAIN_DATASET_PATH\n",
    "test_dataset_path = configs.TEST_DATASET_PATH\n",
    "training_label_name = configs.TRAINING_LABEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train, columns=cpg_sites)\n",
    "train_df[training_label_name] = y_train\n",
    "save_to_gcs(train_df, f\"gs://{BUCKET_NAME}/\", train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(X_test, columns=cpg_sites)\n",
    "test_df[training_label_name] = y_test\n",
    "save_to_gcs(test_df, f\"gs://{BUCKET_NAME}/\", test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start an AI Platform training job, we will first create a Docker image of our code and then use the gcloud command line to start the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_REPO_NAME = configs.IMAGE_REPO_NAME\n",
    "IMAGE_TAG = configs.IMAGE_TAG\n",
    "IMAGE_URI = configs.IMAGE_URI\n",
    "TRAINING_DOCKERFILE = configs.TRAINING_DOCKERFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   1.14GB\n",
      "Step 1/12 : FROM python:3.8.6\n",
      " ---> f5e423f5ce1f\n",
      "Step 2/12 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 2b02197714a0\n",
      "Step 3/12 : RUN pip install xgboost==1.2.1 scikit-learn==0.23.2 pandas==1.1.3 joblib==0.17.0\n",
      " ---> Using cache\n",
      " ---> e7d979897e13\n",
      "Step 4/12 : RUN pip install cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> 0bda83cd5ccf\n",
      "Step 5/12 : RUN pip install google-cloud-storage==1.32.0\n",
      " ---> Using cache\n",
      " ---> 3d3ee957411a\n",
      "Step 6/12 : RUN wget -nv     https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz &&     mkdir /root/tools &&     tar xvzf google-cloud-sdk.tar.gz -C /root/tools &&     rm google-cloud-sdk.tar.gz &&     /root/tools/google-cloud-sdk/install.sh --usage-reporting=false         --path-update=false --bash-completion=false         --disable-installation-options &&     rm -rf /root/.config/* &&     ln -s /root/.config /config &&     rm -rf /root/tools/google-cloud-sdk/.install/.backup\n",
      " ---> Using cache\n",
      " ---> a385905dd329\n",
      "Step 7/12 : ENV PATH $PATH:/root/tools/google-cloud-sdk/bin\n",
      " ---> Using cache\n",
      " ---> b7d60a008d2d\n",
      "Step 8/12 : RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
      " ---> Using cache\n",
      " ---> 0edb1ab439fe\n",
      "Step 9/12 : RUN mkdir /root/trainer\n",
      " ---> Using cache\n",
      " ---> 23fc9149a8c1\n",
      "Step 10/12 : COPY ./ai-platform-training/trainer /root/trainer\n",
      " ---> Using cache\n",
      " ---> b7eecf3a1f67\n",
      "Step 11/12 : COPY ./configs /root/trainer/configs\n",
      " ---> Using cache\n",
      " ---> 2e1a8916ba27\n",
      "Step 12/12 : ENTRYPOINT [\"python\", \"trainer/xgboost-binary-model.py\"]\n",
      " ---> Using cache\n",
      " ---> fef134b9f29b\n",
      "Successfully built fef134b9f29b\n",
      "Successfully tagged gcr.io/gcp-nyc/gcr.io/gcp-nyc/dna-methylation-cancer:xgboost-binary\n"
     ]
    }
   ],
   "source": [
    "!docker build -f {TRAINING_DOCKERFILE} -t {IMAGE_URI} ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we push this image to Google Cloud Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/gcp-nyc/gcr.io/gcp-nyc/dna-methylation-cancer]\n",
      "\n",
      "\u001B[1B4a351004: Preparing \n",
      "\u001B[1B5396cbe7: Preparing \n",
      "\u001B[1Bf1f5e1c8: Preparing \n",
      "\u001B[1Bbcc978e3: Preparing \n",
      "\u001B[1B0dd0e47b: Preparing \n",
      "\u001B[1B53a9bd82: Preparing \n",
      "\u001B[1B8930cc4a: Preparing \n",
      "\u001B[1B261fba2e: Preparing \n",
      "\u001B[1Bc05f6425: Preparing \n",
      "\u001B[1B863eb588: Preparing \n",
      "\u001B[1B381add18: Preparing \n",
      "\u001B[1B43721c9b: Preparing \n",
      "\u001B[5Bc05f6425: Waiting g \n",
      "\u001B[9B53a9bd82: Waiting g \n",
      "\u001B[1B2cfe5c83: Preparing \n",
      "\u001B[1B4f1da707: Preparing \n",
      "\u001B[17Ba351004: Pushed lready exists kBB\u001B[12A\u001B[2K\u001B[10A\u001B[2K\u001B[5A\u001B[2K\u001B[2A\u001B[2K\u001B[16A\u001B[2Kxgboost-binary: digest: sha256:6666a8141f527a54788fb71c35289bd5b2cc8443434a77cb8b732c3b0cee0f4d size: 3894\n"
     ]
    }
   ],
   "source": [
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can push the job to AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f'dna_methylation_xgboost_binary_{date}'\n",
    "MODEL_DIR= f\"gs://{BUCKET_NAME}/job_dir/{date}\"\n",
    "REGION = configs.REGION\n",
    "HPTUNING_CONFIG = configs.HPTUNING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [dna_methylation_xgboost_binary_20201021_212655] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe dna_methylation_xgboost_binary_20201021_212655\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs dna_methylation_xgboost_binary_20201021_212655\n",
      "jobId: dna_methylation_xgboost_binary_20201021_212655\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} --region {REGION} --master-image-uri {IMAGE_URI} --config {HPTUNING_CONFIG} -- --model-dir={MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## AI Platform Prediction\n",
    "\n",
    "The final step will be to deploy our model into AI Platform Prediction\n",
    "to be able to use it in production.\n",
    "\n",
    "For this, we are packaging the code inside `ai-platform-prediction`\n",
    "and saving it to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!cd ai-platform-prediction/prediction_routing/ && python setup.py sdist --format=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=configs.MODEL_NAME\n",
    "VERSION_NAME=configs.VERSION_NAME\n",
    "ORIGIN='gs://dna-methylation-cancer/job_dir/20201021_204518/'\n",
    "PACKAGE_URIS=configs.PACKAGE_URIS\n",
    "PREDICTION_CLASS=configs.PREDICTION_CLASS\n",
    "RUNTIME_VERSION=configs.RUNTIME_VERSION\n",
    "PYTHON_VERSION=configs.PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gsutil cp ai-platform-prediction/prediction_routing/dist/my_custom_code-0.2.tar.gz {PACKAGE_URIS}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a model on AI Platform Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} --regions 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gcloud beta ai-platform versions create {VERSION_NAME} --model {MODEL_NAME} --runtime-version {RUNTIME_VERSION} --python-version {PYTHON_VERSION} --origin {ORIGIN} --package-uris {PACKAGE_URIS} --prediction-class {PREDICTION_CLASS}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing my_custom_code.egg-info/PKG-INFO\n",
      "writing dependency_links to my_custom_code.egg-info/dependency_links.txt\n",
      "writing top-level names to my_custom_code.egg-info/top_level.txt\n",
      "reading manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "writing manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating my_custom_code-0.2\n",
      "creating my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying files to my_custom_code-0.2...\n",
      "copying predictor.py -> my_custom_code-0.2\n",
      "copying setup.py -> my_custom_code-0.2\n",
      "copying my_custom_code.egg-info/PKG-INFO -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/SOURCES.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/dependency_links.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/top_level.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "Writing my_custom_code-0.2/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'my_custom_code-0.2' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd ai-platform-prediction/prediction_routing/ && python setup.py sdist --format=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://ai-platform-prediction/prediction_routing/dist/my_custom_code-0.2.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ai-platform-prediction/prediction_routing/dist/my_custom_code-0.2.tar.gz {PACKAGE_URIS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/gcp-nyc/models/dna_cancer_prediction].\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} --regions 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create {VERSION_NAME} --model {MODEL_NAME} --runtime-version {RUNTIME_VERSION} --python-version {PYTHON_VERSION} --origin {ORIGIN} --package-uris {PACKAGE_URIS} --prediction-class {PREDICTION_CLASS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}