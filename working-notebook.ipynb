{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Cancer with DNA Methylation\n",
    "\n",
    "This notebook is a walk-through of our project submitted to the 2020 Hack for Social Good\n",
    " _Build Hackathon.\n",
    "\n",
    "This notebook is organized the following way:\n",
    " 1. Data preparation\n",
    " 2. Preprocessing\n",
    " 3. Model Training and Evaluation\n",
    " 4. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup\n",
    "This notebook has been tested with Python 3.8.5. If not done already, create the `DNA_Methylation`\n",
    "virtual environment using conda and the `environment.yml` file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "import google.auth\n",
    "\n",
    "from configs import configs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tested this notebook with:\n",
    " - Pandas version used: 1.0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 1.0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas version used: {pd.__version__}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve credentials to use GCP's Python libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcp-nyc\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "print(project_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "PROJECT_ID = configs.PROJECT_ID\n",
    "DATASET = configs.DATASET"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "\n",
    "In the data preparation part, we will create two bigquery tables that will used to create the\n",
    "final train and test datasets.\n",
    "\n",
    "The first BigQuery table is called `tcga_betas` is a BigQuery partitioned and clustered table\n",
    "that will hold all betas values for all TCGA patients. It will be a table in a long format:\n",
    "one row per betas observation.\n",
    "\n",
    "The second BigQuery table is called `columns_to_keep` is a BigQuery table that will hold the\n",
    "list of CpG site that we will keep in the final dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First table - `tcga_betas` table: a simple SQL script\n",
    "\n",
    "This first table will be created using a simple SQL query. The query is located in\n",
    "`SQL_queries/tcga_betas.txt`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE dna_cancer_prediction.tcga_betas\n",
      " PARTITION BY RANGE_BUCKET(row_number, GENERATE_ARRAY(1, 11000, 100))\n",
      " CLUSTER BY row_number\n",
      " AS\n",
      " \n",
      " WITH\n",
      "   brca_betas AS (\n",
      "   SELECT\n",
      "     MIM.CpG_probe_id,\n",
      "     dna.case_barcode,\n",
      "     dna.sample_barcode,\n",
      "     dna.probe_id,\n",
      "     dna.beta_value,\n",
      "     gc.Name\n",
      "   FROM\n",
      "     `isb-cgc.platform_reference.methylation_annotation` gc\n",
      "   JOIN\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` dna\n",
      "   ON\n",
      "     gc.Name = dna.probe_id\n",
      "   JOIN\n",
      "     `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` MIM\n",
      "   ON\n",
      "     MIM.CpG_probe_id = gc.Name),\n",
      " \n",
      "   participants_id AS (\n",
      "   SELECT\n",
      "     DISTINCT case_barcode\n",
      "   FROM\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation`),\n",
      " \n",
      "   participants_id_numbered AS (\n",
      "   SELECT\n",
      "     case_barcode,\n",
      "     ROW_NUMBER() OVER (order by case_barcode) as row_number\n",
      "   FROM\n",
      "     participants_id)\n",
      " \n",
      " SELECT\n",
      "   A.case_barcode,\n",
      "   A.beta_value,\n",
      "   A.CpG_probe_id,\n",
      "   A.sample_barcode,\n",
      "   SUBSTR(A.sample_barcode, 14, 2) AS sample_id,\n",
      "   IF\n",
      "   (SUBSTR(A.sample_barcode, 14, 2) IN ('10', '11', '12', '13', '14', '15', '16', '17', '18', '19'), \"normal\",\n",
      "   IF\n",
      "   (SUBSTR(A.sample_barcode, 14, 2) IN ('01', '02', '03', '04',  '05', '06', '07', '08', '09'), 'tumor', \"control\")) AS sample_status,\n",
      "   B.row_number\n",
      " FROM\n",
      "   brca_betas A\n",
      " LEFT JOIN\n",
      "   participants_id_numbered B\n",
      " ON A.case_barcode = B.case_barcode\n"
     ]
    }
   ],
   "source": [
    "SQL_query_path = 'SQL_queries/tcga_betas.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TABLE_NAME__', 'tcga_betas')\n",
    "print(sql_query)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TO DO: describe query\n",
    "\n",
    "We can execute the job to create the table in BigQuery."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second table - `columns_to_keep`: using dataproc\n",
    "\n",
    "The second table will be created by a Dataflow job. This table will hold the list of CpG sites\n",
    "that will be included in the training dataset.\n",
    "\n",
    "Indeed, we want to keep only the 5,000 best CpG site among the 500,000 that will be able to\n",
    "separate cancerous vs non-cancerous observations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [0f0e58d6ac554f16a14fca15e17a370e] submitted.\n",
      "Waiting for job output...\n",
      "build_hackathon_dnanyc_tfx_pipeline\n",
      "20/10/19 20:01:32 INFO org.spark_project.jetty.util.log: Logging initialized @2641ms\n",
      "20/10/19 20:01:32 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/10/19 20:01:32 INFO org.spark_project.jetty.server.Server: Started @2730ms\n",
      "20/10/19 20:01:32 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@6b8d7fdc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/19 20:01:32 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/10/19 20:01:33 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at build-hackathon-nyc-cluster-m/10.0.0.39:8032\n",
      "20/10/19 20:01:33 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at build-hackathon-nyc-cluster-m/10.0.0.39:10200\n",
      "20/10/19 20:01:35 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1603135504197_0005\n",
      "20/10/19 20:01:42 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[case_barcode], filters=[]\n",
      "20/10/19 20:01:42 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[case_barcode], filter=''\n",
      "20/10/19 20:01:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDDZ6U0FtQ2NVTktxeRoCaXIaAmpk\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.network.client.TransportClient: Failed to send RPC 6122902277242269437 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(339,0,Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Failed to send RPC 6122902277242269437 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 WARN org.apache.spark.ExecutorAllocationManager: Uncaught exception in thread spark-dynamic-executor-allocation\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:567)\n",
      "\tat org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:344)\n",
      "\tat org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:305)\n",
      "\tat org.apache.spark.ExecutorAllocationManager$$anon$2.run(ExecutorAllocationManager.scala:234)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to send RPC 6122902277242269437 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.network.client.TransportClient: Failed to send RPC 6043025206266156205 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 2 at RPC address 10.0.0.99:41142, but got no response. Marking as slave lost.\n",
      "java.io.IOException: Failed to send RPC 6043025206266156205 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on build-hackathon-nyc-cluster-sw-zx2s.c.gcp-nyc.internal: Slave lost\n",
      "20/10/19 20:02:08 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 34.0 in stage 0.0 (TID 34, build-hackathon-nyc-cluster-sw-zx2s.c.gcp-nyc.internal, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Slave lost\n",
      "20/10/19 20:02:08 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 33.0 in stage 0.0 (TID 33, build-hackathon-nyc-cluster-sw-zx2s.c.gcp-nyc.internal, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Slave lost\n",
      "20/10/19 20:02:08 WARN org.apache.spark.ExecutorAllocationManager: Attempted to mark unknown executor 2 idle\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.network.client.TransportClient: Failed to send RPC 5003903889358180000 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(338,0,Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Failed to send RPC 5003903889358180000 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 WARN org.apache.spark.ExecutorAllocationManager: Uncaught exception in thread spark-dynamic-executor-allocation\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:567)\n",
      "\tat org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:344)\n",
      "\tat org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:305)\n",
      "\tat org.apache.spark.ExecutorAllocationManager$$anon$2.run(ExecutorAllocationManager.scala:234)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to send RPC 5003903889358180000 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n",
      "\t... 1 more\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.network.client.TransportClient: Failed to send RPC 5753073163381895826 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)\n",
      "20/10/19 20:02:08 ERROR org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(337,0,Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Failed to send RPC 5753073163381895826 to /10.0.0.99:41132: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient.lambda$sendRpc$2(TransportClient.java:237)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38)\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster build-hackathon-nyc-cluster \\\n",
    "    --region us-east1 --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n",
    "    dataproc_processing/create_cpg_sites_list.py \\\n",
    "    -- -gcs_bucket \"build_hackathon_dnanyc_tfx_pipeline\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating training dataset\n",
    "\n",
    "We will now use those training tables to create our training dataset. The training dataset will be\n",
    "saved into a GCS bucket."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_PATH = configs.DATASET_PATH\n",
    "DATASET_NAME = 'tcga-binary.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def configure_gcs(project_id=PROJECT_ID):\n",
    "    client = storage.Client(project=project_id)\n",
    "    return client\n",
    "\n",
    "def save_to_gcs(df, gcs_path, file_name):\n",
    "    df.to_csv(gcs_path + file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def download_from_bigquery(project_id, list_of_columns):\n",
    "    formatted_columns = \"', '\".join(list_of_columns)\n",
    "    query = f\"\"\"\n",
    "    SELECT beta_value, CpG_probe_id, participant_id, sample_id, aliquot_barcode\n",
    "    from `build_hackathon_dnanyc.brca_betas_clustered_efficient`\n",
    "    where CpG_probe_id in ('{formatted_columns}')\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df\n",
    "\n",
    "def download_columns_to_keep(project_id):\n",
    "    query = \"\"\"\n",
    "    SELECT CpG_probe_id\n",
    "    from `dna_cancer_prediction.cpg_site_list`\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df['CpG_probe_id'].values\n",
    "\n",
    "def merge_and_pivot(df_betas):\n",
    "    df_p = df_betas.pivot(index=\"sample_barcode\", columns='CpG_probe_id',\n",
    "         values='beta_value')\n",
    "    df_p['case_barcode'] = df_p['sample_barcode'].str[:12]\n",
    "    df_final = df_p.merge(df_betas[['case_barcode', 'sample_status']],\n",
    "                          how='left', on='case_barcode')\n",
    "    df_final = df_final.drop('case_barcode', axis=1)\n",
    "    df_final = df_final.set_index('sample_barcode')\n",
    "    return df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrien.galamez\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas_gbq\\gbq.py:559: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  df = rows_iter.to_dataframe(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-a75d8643f272>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mgcs_client\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfigure_gcs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPROJECT_ID\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mcolumns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdownload_columns_to_keep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPROJECT_ID\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdownload_from_bigquery\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mPROJECT_ID\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-10-5dfe40a9c07b>\u001B[0m in \u001B[0;36mdownload_from_bigquery\u001B[1;34m(project_id, list_of_columns)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mwhere\u001B[0m \u001B[0mCpG_probe_id\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'{formatted_columns}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \"\"\"\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_gbq\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquery\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproject_id\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mproject_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas\\io\\gbq.py\u001B[0m in \u001B[0;36mread_gbq\u001B[1;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, private_key, verbose, progress_bar_type)\u001B[0m\n\u001B[0;32m    173\u001B[0m     \u001B[1;31m# END: new kwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m     return pandas_gbq.read_gbq(\n\u001B[0m\u001B[0;32m    176\u001B[0m         \u001B[0mquery\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m         \u001B[0mproject_id\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mproject_id\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas_gbq\\gbq.py\u001B[0m in \u001B[0;36mread_gbq\u001B[1;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, verbose, private_key, progress_bar_type)\u001B[0m\n\u001B[0;32m    961\u001B[0m     )\n\u001B[0;32m    962\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 963\u001B[1;33m     final_df = connector.run_query(\n\u001B[0m\u001B[0;32m    964\u001B[0m         \u001B[0mquery\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    965\u001B[0m         \u001B[0mconfiguration\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfiguration\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas_gbq\\gbq.py\u001B[0m in \u001B[0;36mrun_query\u001B[1;34m(self, query, max_results, progress_bar_type, **kwargs)\u001B[0m\n\u001B[0;32m    527\u001B[0m             )\n\u001B[0;32m    528\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 529\u001B[1;33m         return self._download_results(\n\u001B[0m\u001B[0;32m    530\u001B[0m             \u001B[0mquery_reply\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m             \u001B[0mmax_results\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_results\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas_gbq\\gbq.py\u001B[0m in \u001B[0;36m_download_results\u001B[1;34m(self, query_job, max_results, progress_bar_type)\u001B[0m\n\u001B[0;32m    557\u001B[0m             \u001B[0mschema_fields\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfield\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_api_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mfield\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrows_iter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    558\u001B[0m             \u001B[0mnullsafe_dtypes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_bqschema_to_nullsafe_dtypes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mschema_fields\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 559\u001B[1;33m             df = rows_iter.to_dataframe(\n\u001B[0m\u001B[0;32m    560\u001B[0m                 \u001B[0mdtypes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnullsafe_dtypes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    561\u001B[0m                 \u001B[0mbqstorage_client\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbqstorage_client\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\table.py\u001B[0m in \u001B[0;36mto_dataframe\u001B[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object)\u001B[0m\n\u001B[0;32m   1774\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1775\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1776\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mframe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_dataframe_iterable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1777\u001B[0m             \u001B[0mframes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1778\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\table.py\u001B[0m in \u001B[0;36m_to_page_iterable\u001B[1;34m(self, bqstorage_download, tabledata_list_download, bqstorage_client)\u001B[0m\n\u001B[0;32m   1437\u001B[0m             \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1439\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtabledata_list_download\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1440\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1441\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py\u001B[0m in \u001B[0;36mdownload_dataframe_tabledata_list\u001B[1;34m(pages, bq_schema, dtypes)\u001B[0m\n\u001B[0;32m    563\u001B[0m     \u001B[0mcolumn_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfield\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mfield\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbq_schema\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    564\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mpage\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpages\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 565\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[0m_tabledata_list_page_to_dataframe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    566\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py\u001B[0m in \u001B[0;36m_tabledata_list_page_to_dataframe\u001B[1;34m(page, column_names, dtypes)\u001B[0m\n\u001B[0;32m    537\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mcolumn_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumn_names\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    538\u001B[0m         \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 539\u001B[1;33m         \u001B[0mcolumns\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_columns\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcolumn_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    540\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    541\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcolumn_names\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[0;32m    277\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_dense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 279\u001B[1;33m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaybe_iterable_to_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    280\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    281\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\pandas\\core\\common.py\u001B[0m in \u001B[0;36mmaybe_iterable_to_list\u001B[1;34m(obj)\u001B[0m\n\u001B[0;32m    278\u001B[0m     \"\"\"\n\u001B[0;32m    279\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mabc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mabc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSized\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 280\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    281\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\table.py\u001B[0m in \u001B[0;36mget_column_data\u001B[1;34m(field_index, field)\u001B[0m\n\u001B[0;32m   2223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_column_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfield_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2224\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mrow\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrows\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2225\u001B[1;33m             \u001B[1;32myield\u001B[0m \u001B[0m_helpers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_field_from_json\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"f\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfield_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"v\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2226\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2227\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mfield_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\_helpers.py\u001B[0m in \u001B[0;36m_field_from_json\u001B[1;34m(resource, field)\u001B[0m\n\u001B[0;32m    216\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mconverter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"v\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mresource\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 218\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mconverter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    219\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gcs_client = configure_gcs(PROJECT_ID)\n",
    "columns = download_columns_to_keep(PROJECT_ID)\n",
    "df = download_from_bigquery(PROJECT_ID, columns)\n",
    "df = merge_and_pivot(df)\n",
    "save_to_gcs(df, DATASET_PATH, DATASET_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}