{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Cancer with DNA Methylation\n",
    "\n",
    "This notebook is a walk-through of our project submitted to the 2020 Hack for Social Good\n",
    " _Build Hackathon.\n",
    "\n",
    "This notebook is organized the following way:\n",
    " 1. Data preparation\n",
    " 2. Preprocessing\n",
    " 3. Model Training and Evaluation\n",
    " 4. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment setup\n",
    "This notebook has been tested with Python 3.8.5. If not done already, create the `DNA_Methylation`\n",
    "virtual environment using conda and the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "import google.auth\n",
    "\n",
    "from configs import configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We tested this notebook with:\n",
    " - Pandas version used: 1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas version used: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Retrieve credentials to use GCP's Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcp-nyc\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = configs.PROJECT_ID\n",
    "DATASET = configs.DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "In the data preparation part, we will create two bigquery tables that will used to create the\n",
    "final train and test datasets.\n",
    "\n",
    "The first BigQuery table is called `tcga_betas` is a BigQuery partitioned and clustered table\n",
    "that will hold all betas values for all TCGA patients. It will be a table in a long format:\n",
    "one row per betas observation.\n",
    "\n",
    "The second BigQuery table is called `columns_to_keep` is a BigQuery table that will hold the\n",
    "list of CpG site that we will keep in the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First table - `tcga_betas` table: a simple SQL script\n",
    "\n",
    "This first table will be created using a simple SQL query. The query is located in\n",
    "`SQL_queries/tcga_betas.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE dna_cancer_prediction.tcga_betas\n",
      " PARTITION BY RANGE_BUCKET(row_number, GENERATE_ARRAY(1, 11000, 100))\n",
      " CLUSTER BY row_number\n",
      " AS\n",
      " \n",
      " WITH\n",
      "   brca_betas AS (\n",
      "   SELECT\n",
      "     MIM.CpG_probe_id,\n",
      "     dna.case_barcode,\n",
      "     dna.aliquot_barcode,\n",
      "     dna.probe_id,\n",
      "     dna.beta_value,\n",
      "     gc.Name\n",
      "   FROM\n",
      "     `isb-cgc.platform_reference.methylation_annotation` gc\n",
      "   JOIN\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` dna\n",
      "   ON\n",
      "     gc.Name = dna.probe_id\n",
      "   JOIN\n",
      "     `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` MIM\n",
      "   ON\n",
      "     MIM.CpG_probe_id = gc.Name),\n",
      " \n",
      "   participants_id AS (\n",
      "   SELECT\n",
      "     DISTINCT case_barcode\n",
      "   FROM\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation`),\n",
      " \n",
      "   participants_id_numbered AS (\n",
      "   SELECT\n",
      "     case_barcode,\n",
      "     ROW_NUMBER() OVER (order by case_barcode) as row_number\n",
      "   FROM\n",
      "     participants_id)\n",
      " \n",
      " SELECT\n",
      "   A.case_barcode,\n",
      "   A.beta_value,\n",
      "   A.CpG_probe_id,\n",
      "   A.aliquot_barcode,\n",
      "   SUBSTR(A.aliquot_barcode, 14, 2) AS sample_id,\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('10', '11', '12', '13', '14', '15', '16', '17', '18', '19'), \"normal\",\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('01', '02', '03', '04',  '05', '06', '07', '08', '09'), 'tumor', \"control\")) AS sample_status,\n",
      "   B.row_number\n",
      " FROM\n",
      "   brca_betas A\n",
      " LEFT JOIN\n",
      "   participants_id_numbered B\n",
      " ON A.case_barcode = B.case_barcode\n"
     ]
    }
   ],
   "source": [
    "SQL_query_path = 'sql_queries/tcga_betas.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TABLE_NAME__', 'tcga_betas')\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TO DO: describe query\n",
    "\n",
    "We can execute the job to create the table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Second table - `columns_to_keep`: using dataproc\n",
    "\n",
    "The second table will be created by a Dataflow job. This table will hold the list of CpG sites\n",
    "that will be included in the training dataset.\n",
    "\n",
    "Indeed, we want to keep only the 5,000 best CpG site among the 500,000 that will be able to\n",
    "separate cancerous vs non-cancerous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [81fa880ca37c4901bc736ce3e941c5ea] submitted.\n",
      "Waiting for job output...\n",
      "build_hackathon_dnanyc_tfx_pipeline\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.util.log: Logging initialized @2728ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: Started @2816ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:20:36 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at build-hackathon-nyc-cluster-m/10.0.0.39:8032\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at build-hackathon-nyc-cluster-m/10.0.0.39:10200\n",
      "20/10/20 14:20:39 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1603203106846_0002\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[aliquot_barcode], filters=[]\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[aliquot_barcode], filter=''\n",
      "20/10/20 14:20:50 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGRFamNsRlVxTHJXNBoCaXIaAmpk\n",
      "20/10/20 14:25:00 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'CpG_probe_id#2 = CpG_probe_id#2'. Perhaps you need to use aliases.\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id,sample_status], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id, sample_status], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEtCZlRZTWF4c0I3VxoCaXIaAmpk\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[CpG_probe_id], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[CpG_probe_id], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGxYLTZ2RWlsaEc4YhoCaXIaAmpk\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id], filters=[]\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id], filter=''\n",
      "20/10/20 14:25:07 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEwwSkt5V2hqQi1lcBoCaXIaAmpk\n",
      "20/10/20 14:35:11 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=dna_cancer_prediction, projectId=gcp-nyc, tableId=cpg_site_list}}. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to gcp-nyc.dna_cancer_prediction.cpg_site_list. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.deploy.yarn.Client: Failed to contact YARN for application application_1603203106846_0002.\n",
      "java.io.InterruptedIOException: Call interrupted\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1504)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1456)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1366)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:232)\n",
      "\tat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy16.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:496)\n",
      "\tat org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:301)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1060)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:109)\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend: Yarn application has already exited with state FAILED!\n",
      "Job [81fa880ca37c4901bc736ce3e941c5ea] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/\n",
      "driverOutputResourceUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/driveroutput\n",
      "jobUuid: 96de5e5c-9a94-3efb-98d4-15dc226da94f\n",
      "placement:\n",
      "  clusterName: build-hackathon-nyc-cluster\n",
      "  clusterUuid: 11f36b0a-7180-44e2-8f22-a219c38293fb\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - -gcs_bucket\n",
      "  - build_hackathon_dnanyc_tfx_pipeline\n",
      "  jarFileUris:\n",
      "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
      "  mainPythonFileUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/staging/create_cpg_sites_list.py\n",
      "reference:\n",
      "  jobId: 81fa880ca37c4901bc736ce3e941c5ea\n",
      "  projectId: gcp-nyc\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2020-10-20T14:35:23.713Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2020-10-20T14:20:31.861Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2020-10-20T14:20:31.926Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2020-10-20T14:20:32.149Z'\n",
      "yarnApplications:\n",
      "- name: my_app\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://build-hackathon-nyc-cluster-m:8088/proxy/application_1603203106846_0002/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster build-hackathon-nyc-cluster \\\n",
    "    --region us-east1 --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n",
    "    dataproc_processing/create_cpg_sites_list.py \\\n",
    "    -- -gcs_bucket \"build_hackathon_dnanyc_tfx_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating training dataset\n",
    "\n",
    "We will now use those training tables to create our training dataset. The training dataset will be\n",
    "saved into a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = configs.DATASET_PATH\n",
    "DATASET_NAME = 'tcga-binary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def configure_gcs(project_id=PROJECT_ID):\n",
    "    client = storage.Client(project=project_id)\n",
    "    return client\n",
    "\n",
    "def save_to_gcs(df, gcs_path, file_name):\n",
    "    df.to_csv(gcs_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_bigquery(project_id, list_of_columns):\n",
    "    formatted_columns = \"', '\".join(list_of_columns)\n",
    "    query = f\"\"\"\n",
    "    SELECT beta_value, CpG_probe_id, aliquot_barcode, sample_status\n",
    "    from `dna_cancer_prediction.tcga_betas`\n",
    "    where CpG_probe_id in ('{formatted_columns}')\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df\n",
    "\n",
    "def download_columns_to_keep(project_id):\n",
    "    query = \"\"\"\n",
    "    SELECT CpG_probe_id\n",
    "    from `dna_cancer_prediction.cpg_site_list`\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df['CpG_probe_id'].values\n",
    "\n",
    "def merge_and_pivot(df_betas):\n",
    "    # TO DO: use aliquot_barcode instead of sample barcode as identifier\n",
    "#     df_betas = df_betas.drop_duplicates(subset=['sample_barcode', 'CpG_probe_id'])\n",
    "    df_p = df_betas.pivot(index=\"aliquot_barcode\", columns='CpG_probe_id', values='beta_value').reset_index()\n",
    "    df_final = df_p.merge(df_betas[['aliquot_barcode', 'sample_status']],\n",
    "                          how='left', on='aliquot_barcode')\n",
    "    df_final = df_final.set_index('aliquot_barcode')\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gcs_client = configure_gcs(PROJECT_ID)\n",
    "columns = download_columns_to_keep(PROJECT_ID)\n",
    "df = download_from_bigquery(PROJECT_ID, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_and_pivot(df)\n",
    "save_to_gcs(df, DATASET_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tumor'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sample_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
