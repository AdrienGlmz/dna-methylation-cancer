{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Cancer with DNA Methylation\n",
    "\n",
    "This notebook is a walk-through of our project submitted to the 2020 Hack for Social Good\n",
    " _Build Hackathon.\n",
    "\n",
    "This notebook is organized the following way:\n",
    " 1. Data preparation\n",
    " 2. Preprocessing\n",
    " 3. Model Training and Evaluation\n",
    " 4. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment setup\n",
    "This notebook has been tested with Python 3.8.5. If not done already, create the `DNA_Methylation`\n",
    "virtual environment using conda and the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, \\\n",
    "                           plot_confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from feature_engineering.get_data import read_dataset, read_from_gcs, \\\n",
    "    download_data, configure_gcs\n",
    "from feature_engineering.preprocessing import preprocessing\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "import google.auth\n",
    "\n",
    "from configs import configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We tested this notebook with:\n",
    " - Pandas version used: 1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas version used: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Retrieve credentials to use GCP's Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcp-nyc\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = configs.PROJECT_ID\n",
    "DATASET = configs.DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "In the data preparation part, we will create two bigquery tables that will used to create the\n",
    "final train and test datasets.\n",
    "\n",
    "The first BigQuery table is called `tcga_betas` is a BigQuery partitioned and clustered table\n",
    "that will hold all betas values for all TCGA patients. It will be a table in a long format:\n",
    "one row per betas observation.\n",
    "\n",
    "The second BigQuery table is called `columns_to_keep` is a BigQuery table that will hold the\n",
    "list of CpG site that we will keep in the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First table - `tcga_betas` table: a simple SQL script\n",
    "\n",
    "This first table will be created using a simple SQL query. The query is located in\n",
    "`SQL_queries/tcga_betas.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE dna_cancer_prediction.tcga_betas\n",
      " PARTITION BY RANGE_BUCKET(row_number, GENERATE_ARRAY(1, 11000, 100))\n",
      " CLUSTER BY row_number\n",
      " AS\n",
      " \n",
      " WITH\n",
      "   brca_betas AS (\n",
      "   SELECT\n",
      "     MIM.CpG_probe_id,\n",
      "     dna.case_barcode,\n",
      "     dna.aliquot_barcode,\n",
      "     dna.probe_id,\n",
      "     dna.beta_value,\n",
      "     gc.Name\n",
      "   FROM\n",
      "     `isb-cgc.platform_reference.methylation_annotation` gc\n",
      "   JOIN\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` dna\n",
      "   ON\n",
      "     gc.Name = dna.probe_id\n",
      "   JOIN\n",
      "     `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` MIM\n",
      "   ON\n",
      "     MIM.CpG_probe_id = gc.Name),\n",
      " \n",
      "   participants_id AS (\n",
      "   SELECT\n",
      "     DISTINCT case_barcode\n",
      "   FROM\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation`),\n",
      " \n",
      "   participants_id_numbered AS (\n",
      "   SELECT\n",
      "     case_barcode,\n",
      "     ROW_NUMBER() OVER (order by case_barcode) as row_number\n",
      "   FROM\n",
      "     participants_id)\n",
      " \n",
      " SELECT\n",
      "   A.case_barcode,\n",
      "   A.beta_value,\n",
      "   A.CpG_probe_id,\n",
      "   A.aliquot_barcode,\n",
      "   SUBSTR(A.aliquot_barcode, 14, 2) AS sample_id,\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('10', '11', '12', '13', '14', '15', '16', '17', '18', '19'), \"normal\",\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('01', '02', '03', '04',  '05', '06', '07', '08', '09'), 'tumor', \"control\")) AS sample_status,\n",
      "   B.row_number\n",
      " FROM\n",
      "   brca_betas A\n",
      " LEFT JOIN\n",
      "   participants_id_numbered B\n",
      " ON A.case_barcode = B.case_barcode\n"
     ]
    }
   ],
   "source": [
    "SQL_query_path = 'sql_queries/tcga_betas.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TABLE_NAME__', 'tcga_betas')\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TO DO: describe query\n",
    "\n",
    "We can execute the job to create the table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Second table - `columns_to_keep`: using dataproc\n",
    "\n",
    "The second table will be created by a Dataflow job. This table will hold the list of CpG sites\n",
    "that will be included in the training dataset.\n",
    "\n",
    "Indeed, we want to keep only the 5,000 best CpG site among the 500,000 that will be able to\n",
    "separate cancerous vs non-cancerous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [81fa880ca37c4901bc736ce3e941c5ea] submitted.\n",
      "Waiting for job output...\n",
      "build_hackathon_dnanyc_tfx_pipeline\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.util.log: Logging initialized @2728ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: Started @2816ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:20:36 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at build-hackathon-nyc-cluster-m/10.0.0.39:8032\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at build-hackathon-nyc-cluster-m/10.0.0.39:10200\n",
      "20/10/20 14:20:39 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1603203106846_0002\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[aliquot_barcode], filters=[]\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[aliquot_barcode], filter=''\n",
      "20/10/20 14:20:50 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGRFamNsRlVxTHJXNBoCaXIaAmpk\n",
      "20/10/20 14:25:00 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'CpG_probe_id#2 = CpG_probe_id#2'. Perhaps you need to use aliases.\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id,sample_status], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id, sample_status], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEtCZlRZTWF4c0I3VxoCaXIaAmpk\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[CpG_probe_id], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[CpG_probe_id], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGxYLTZ2RWlsaEc4YhoCaXIaAmpk\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id], filters=[]\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id], filter=''\n",
      "20/10/20 14:25:07 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEwwSkt5V2hqQi1lcBoCaXIaAmpk\n",
      "20/10/20 14:35:11 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=dna_cancer_prediction, projectId=gcp-nyc, tableId=cpg_site_list}}. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to gcp-nyc.dna_cancer_prediction.cpg_site_list. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.deploy.yarn.Client: Failed to contact YARN for application application_1603203106846_0002.\n",
      "java.io.InterruptedIOException: Call interrupted\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1504)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1456)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1366)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:232)\n",
      "\tat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy16.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:496)\n",
      "\tat org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:301)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1060)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:109)\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend: Yarn application has already exited with state FAILED!\n",
      "Job [81fa880ca37c4901bc736ce3e941c5ea] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/\n",
      "driverOutputResourceUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/driveroutput\n",
      "jobUuid: 96de5e5c-9a94-3efb-98d4-15dc226da94f\n",
      "placement:\n",
      "  clusterName: build-hackathon-nyc-cluster\n",
      "  clusterUuid: 11f36b0a-7180-44e2-8f22-a219c38293fb\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - -gcs_bucket\n",
      "  - build_hackathon_dnanyc_tfx_pipeline\n",
      "  jarFileUris:\n",
      "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
      "  mainPythonFileUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/staging/create_cpg_sites_list.py\n",
      "reference:\n",
      "  jobId: 81fa880ca37c4901bc736ce3e941c5ea\n",
      "  projectId: gcp-nyc\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2020-10-20T14:35:23.713Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2020-10-20T14:20:31.861Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2020-10-20T14:20:31.926Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2020-10-20T14:20:32.149Z'\n",
      "yarnApplications:\n",
      "- name: my_app\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://build-hackathon-nyc-cluster-m:8088/proxy/application_1603203106846_0002/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster build-hackathon-nyc-cluster \\\n",
    "    --region us-east1 --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n",
    "    dataproc_processing/create_cpg_sites_list.py \\\n",
    "    -- -gcs_bucket \"build_hackathon_dnanyc_tfx_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating training dataset\n",
    "\n",
    "We will now use those training tables to create our training dataset. The training dataset will be\n",
    "saved into a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = configs.DATASET_PATH\n",
    "DATASET_NAME = 'tcga-binary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def configure_gcs(project_id=PROJECT_ID):\n",
    "    client = storage.Client(project=project_id)\n",
    "    return client\n",
    "\n",
    "def save_to_gcs(df, gcs_path, file_name):\n",
    "    df.to_csv(gcs_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_bigquery(project_id, dataset, table, list_of_columns, start_index, end_index):\n",
    "    formatted_columns = \"', '\".join(list_of_columns)\n",
    "    query = f\"\"\"\n",
    "    SELECT beta_value, CpG_probe_id, aliquot_barcode, sample_status\n",
    "    from `{dataset}.{table}`\n",
    "    where CpG_probe_id in ('{formatted_columns}') and row_number >= {start_index} and row_number <= {end_index}\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df\n",
    "\n",
    "def download_columns_to_keep(project_id, dataset, table):\n",
    "    query = f\"\"\"\n",
    "    SELECT CpG_probe_id\n",
    "    from `{dataset}.{table}`\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df['CpG_probe_id'].values\n",
    "\n",
    "def merge_and_pivot_by_slices(project_id, dataset, table, columns, start_index, end_index):\n",
    "    df_betas = download_from_bigquery(project_id, dataset, table, columns, start_index, end_index)\n",
    "    df_p = df_betas.pivot(index=\"aliquot_barcode\", columns='CpG_probe_id', values='beta_value').reset_index()\n",
    "    df_labels = df_betas[['aliquot_barcode', 'sample_status']].drop_duplicates()\n",
    "    df_final = df_p.merge(df_labels,\n",
    "                          how='left', on='aliquot_barcode')\n",
    "    df_final = df_final.set_index('aliquot_barcode')\n",
    "    # Reorder columns\n",
    "    df_final = df_final[columns]\n",
    "    return df_final\n",
    "\n",
    "def pivot_data(project_id, dataset, betas_table, cpg_site_list_table, max_index=11000, steps=1000):\n",
    "    columns = download_columns_to_keep(project_id, dataset, cpg_site_list_table)\n",
    "    df = merge_and_pivot_by_slices(project_id, dataset, betas_table, columns, 1, steps)\n",
    "    start_index = steps + 1\n",
    "    for i in range(start_index, max_index, steps):\n",
    "        print(f\"Processing index {i} to {i + steps}\")\n",
    "        new_df = merge_and_pivot_by_slices(project_id, dataset, betas_table, columns, i, i + steps)\n",
    "        df = df.append(new_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gcs_client = configure_gcs(PROJECT_ID)\n",
    "columns = download_columns_to_keep(PROJECT_ID, DATASET, 'cpg_site_list')\n",
    "df = pivot_data(PROJECT_ID, DATASET, 'tcga_betas', 'cpg_site_list')\n",
    "save_to_gcs(df, DATASET_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Training\n",
    "\n",
    "Using the saved dataset in Google Cloud Storage, we will now preprocess the dataset to create\n",
    "a train and test split, train our model on the training set and output and evaluation metric\n",
    "on the evaluation set.\n",
    "\n",
    "We chose to an XGBoost model as part of this project. It's a model that first gives a very good\n",
    "accuracy on complex problems. The model is also easy to interpret as it is a tree based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read data from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob training_data/tcga-binary.csv downloaded to training_data.csv.\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = configs.GCS_BUCKET\n",
    "RAW_DATASET_PATH = configs.RAW_DATASET_PATH\n",
    "# TO DO: update label and index names\n",
    "LABELS = 'sample_status'\n",
    "INDEX = 'aliquot_barcode'\n",
    "betas, labels, cpg_sites, index = read_from_gcs(BUCKET_NAME, RAW_DATASET_PATH, INDEX, LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Map classes to a binary integer classification. 1 will represent a tumorous observation and 0 will represent a normal\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map_labels_to_classes = {\n",
    "  'tumor': 1,\n",
    "  'normal': 0\n",
    "}\n",
    "labels = [map_labels_to_classes[elt] for elt in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Re-balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Drop Columns and Rows ===\n",
      "Dropping 0 because of missing labels\n",
      "New Shape = (12298, 5000)\n",
      "Dropping columns which have more than 10% of values missing\n",
      "0 columns will be dropped.\n",
      "betas: New shape is (12298, 5000)\n",
      "cpg_sites: New shape is (5000,)\n",
      "\n",
      "Dropping rows which have more than 10% of values missing\n",
      "We will drop 0 rows\n",
      "betas: New shape is (12298, 5000)\n",
      "labels: New shape is (12298,)\n",
      "\n",
      "=== Fill remaining NAs ===\n",
      "Filling remaining NA values using a KNNImputer\n",
      "38743 NA were filled, i.e. approximately 3.15 per rows\n",
      "\n",
      "=== Train / Test Split ===\n",
      "Splitting dataset into train and test\n",
      "Train = 70 %\n",
      "Test = 30 %\n",
      "\n",
      "=== Standardize dataset ===\n",
      "The average of column mean on train is 0.00\n",
      "The average of column mean on test is 0.01\n",
      "\n",
      "=== Balance dataset with oversample ===\n",
      "[(0, 786), (1, 7822)]\n",
      "The resampling_strategy gives the following repartition {0: 2346, 1: 7822}\n",
      "1560 rows were added in the training data\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, labels, cpg_sites = preprocessing(betas, labels, cpg_sites, index, smote=True,\n",
    "                                                                   fill_na_strategy='knn', sampling_strategy=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_train))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(bargroupgap=0.2,\n",
    "                  title=\"Histogram of training labels after rebalancing the dataset\",\n",
    "                  xaxis_title=\"Cancerous or normal observations\",\n",
    "                  yaxis_title=\"Count of observations\")\n",
    "fig.update_layout(\n",
    "    margin=dict(\n",
    "        l=40,\n",
    "        r=40,\n",
    "        b=40,\n",
    "        t=40,\n",
    "        pad=4\n",
    "    ),\n",
    "    title={\n",
    "        'x':0.5})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have chosen XGBoost, we need to find the hyper-parameters that will provide the best accuracy on\n",
    "our dataset.\n",
    "To do this, we are defining a Stratified-KFold crossvalidation. Stratified means that we are keeping the same\n",
    "proportion of positive and negative observations in each validation fold.\n",
    "Here, we choose to create 4 folds. This means that we will do four fits: each time, training on 3/4 of the training\n",
    "data while testing on the remaining third.\n",
    "After all these fits, we average the F1 score we got and take the hyper-parameter combination that gives the best\n",
    "average F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's explain the XGBoost parameters we are using:\n",
    " - `objective=binary:logistic`: this is a binary classification problem, and we want to ouput the probability that\n",
    "   an observation is cancerous\n",
    " - `colsample_bytree=0.8`: we are considering only 80% of features to build each tree. This prevents over-fitting\n",
    " - `learning_rate=0.1`: prevents over-fitting by shrinking the feature weights\n",
    " - `subsample=0.8`: we are considering only 80% of the observations to build each tree. This prevents over-fitting.\n",
    " - `nthread=4`: uses 4 different thread to train the model. Enables parallel processing.\n",
    "\n",
    "In our parameter grid, we are testing the following parameters:\n",
    " - `max_depth`: the maximum depth of each tree in the model. The higher the parameter, the higher the variance\n",
    " - `min_child_weight`: the minimum sum of hessian weight needed to create a child node and partition further the tree.\n",
    "   The higher the parameter, the lower the variance.\n",
    " - `gamma`: the minimum loss reduction required to make a further partition on a leaf node. The higher the parameter,\n",
    "   the lower the variance.\n",
    " - `alpha`: L1 regularisation term on weights. The higher the parameter, the lower the variance.\n",
    " - `n_estimator`: number of trees in the model. The higher the parameter, the higher the variance.\n",
    "\n",
    "See [here](https://xgboost.readthedocs.io/en/latest/parameter.html) for the complete XGBoost documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': range(3,10,2),\n",
    "    'min_child_weight': range(1,6,2),\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'alpha':[6,8,10,12],\n",
    "    'n_estimators': [1e2, 1e3, 1e4]\n",
    "}\n",
    "estimator = xgb.XGBClassifier(objective= 'binary:logistic',\n",
    "                              colsample_bytree = 0.8,\n",
    "                              learning_rate = 0.1,\n",
    "                              subsample=0.8,\n",
    "                              nthread=4)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid = param_test1,\n",
    "                        scoring='f1_weighted',n_jobs=4, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's retrieve the best parameters from the GridSearch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_min_child_weight = best_params['min_child_weight']\n",
    "best_gamma = best_params['gamma']\n",
    "best_alpha = best_params['alpha']\n",
    "best_n_estimators = best_params['n_estimators']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can fit the model with the optimized hyper-parameters on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                           colsample_bytree = 0.8,\n",
    "                           learning_rate = 0.1,\n",
    "                           subsample=0.8,\n",
    "                           nthread=4,\n",
    "                           max_depth=10,\n",
    "                           gamma=0.1,\n",
    "                           alpha=6,\n",
    "                           n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's now analyze the model performance\n",
    "\n",
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the f1 score as a rough proxy for model performance. First, on the training set and then on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_predicted_labels = model.predict(X_train)\n",
    "f1_training = f1_score(training_predicted_labels, y_train)\n",
    "print(f\"The train accuracy is {f1_training:.3f}\")\n",
    "\n",
    "# Testing\n",
    "testing_predicted_labels = model.predict(X_test)\n",
    "f1_testing = f1_score(testing_predicted_labels, y_test)\n",
    "print(f\"The test accuracy is {f1_testing:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "To go one level deeper, we can output the confusion matrix on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the *training* set\")\n",
    "plot_confusion_matrix(model, X_train, y_train, ax=ax, values_format='.0f', normalize=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the test set\")\n",
    "plot_confusion_matrix(model, X_test, y_test, ax=ax, values_format='.0f', normalize=None)\n",
    "f.tight_layout()\n",
    "f.savefig(\"confusion_matrix.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training and deploying models on GCP\n",
    "\n",
    "We have trained our XGBoost model locally on this notebook. But, in a production example\n",
    "we would like to better optimize the hyper-parameter by giving more options. We will do\n",
    "this by using the AI Platform Training service.\n",
    "Once this is done, we also need to deploy our model into production. That will be done\n",
    "by the AI Platform Prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first save to GCS the train and test datasets we used earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: replace by configs variables\n",
    "train_dataset_path = 'training_data/train.csv'\n",
    "test_dataset_path = 'training_data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train, columns=cpg_sites)\n",
    "# TO DO: replace by TRAINING_LABEL_NAME\n",
    "train_df['labels'] = y_train\n",
    "save_to_gcs(train_df, f\"gs://{BUCKET_NAME}/\", train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(X_test, columns=cpg_sites)\n",
    "# TO DO: replace by TRAINING_LABEL_NAME\n",
    "test_df['labels'] = y_test\n",
    "save_to_gcs(test_df, f\"gs://{BUCKET_NAME}/\", test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start an AI Platform training job, we will first create a Docker image of our code and then use the gcloud command line to start the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Add to configs file\n",
    "IMAGE_REPO_NAME = 'gcr.io/gcp-nyc/dna-methylation-cancer'\n",
    "IMAGE_TAG = 'xgboost-binary'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_REPO_NAME}:{IMAGE_TAG}\"\n",
    "TRAINING_DOCKERFILE = 'ai-platform-training/Dockerfile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   1.14GB\n",
      "Step 1/12 : FROM python:3.8.6\n",
      " ---> f5e423f5ce1f\n",
      "Step 2/12 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 2b02197714a0\n",
      "Step 3/12 : RUN pip install xgboost==1.2.1 scikit-learn==0.23.2 pandas==1.1.3 joblib==0.17.0\n",
      " ---> Using cache\n",
      " ---> e7d979897e13\n",
      "Step 4/12 : RUN pip install cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> 0bda83cd5ccf\n",
      "Step 5/12 : RUN pip install google-cloud-storage==1.32.0\n",
      " ---> Using cache\n",
      " ---> 3d3ee957411a\n",
      "Step 6/12 : RUN wget -nv     https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz &&     mkdir /root/tools &&     tar xvzf google-cloud-sdk.tar.gz -C /root/tools &&     rm google-cloud-sdk.tar.gz &&     /root/tools/google-cloud-sdk/install.sh --usage-reporting=false         --path-update=false --bash-completion=false         --disable-installation-options &&     rm -rf /root/.config/* &&     ln -s /root/.config /config &&     rm -rf /root/tools/google-cloud-sdk/.install/.backup\n",
      " ---> Using cache\n",
      " ---> a385905dd329\n",
      "Step 7/12 : ENV PATH $PATH:/root/tools/google-cloud-sdk/bin\n",
      " ---> Using cache\n",
      " ---> b7d60a008d2d\n",
      "Step 8/12 : RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
      " ---> Using cache\n",
      " ---> 0edb1ab439fe\n",
      "Step 9/12 : RUN mkdir /root/trainer\n",
      " ---> Using cache\n",
      " ---> 23fc9149a8c1\n",
      "Step 10/12 : COPY ./ai-platform-training/trainer /root/trainer\n",
      " ---> Using cache\n",
      " ---> b7eecf3a1f67\n",
      "Step 11/12 : COPY ./configs /root/trainer/configs\n",
      " ---> Using cache\n",
      " ---> 2e1a8916ba27\n",
      "Step 12/12 : ENTRYPOINT [\"python\", \"trainer/xgboost-binary-model.py\"]\n",
      " ---> Using cache\n",
      " ---> fef134b9f29b\n",
      "Successfully built fef134b9f29b\n",
      "Successfully tagged gcr.io/gcp-nyc/gcr.io/gcp-nyc/dna-methylation-cancer:xgboost-binary\n"
     ]
    }
   ],
   "source": [
    "!docker build -f {TRAINING_DOCKERFILE} -t {IMAGE_URI} ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we push this image to Google Cloud Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/gcp-nyc/gcr.io/gcp-nyc/dna-methylation-cancer]\n",
      "\n",
      "\u001b[1B4a351004: Preparing \n",
      "\u001b[1B5396cbe7: Preparing \n",
      "\u001b[1Bf1f5e1c8: Preparing \n",
      "\u001b[1Bbcc978e3: Preparing \n",
      "\u001b[1B0dd0e47b: Preparing \n",
      "\u001b[1B53a9bd82: Preparing \n",
      "\u001b[1B8930cc4a: Preparing \n",
      "\u001b[1B261fba2e: Preparing \n",
      "\u001b[1Bc05f6425: Preparing \n",
      "\u001b[1B863eb588: Preparing \n",
      "\u001b[1B381add18: Preparing \n",
      "\u001b[1B43721c9b: Preparing \n",
      "\u001b[5Bc05f6425: Waiting g \n",
      "\u001b[9B53a9bd82: Waiting g \n",
      "\u001b[1B2cfe5c83: Preparing \n",
      "\u001b[1B4f1da707: Preparing \n",
      "\u001b[17Ba351004: Pushed lready exists kBB\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[16A\u001b[2Kxgboost-binary: digest: sha256:6666a8141f527a54788fb71c35289bd5b2cc8443434a77cb8b732c3b0cee0f4d size: 3894\n"
     ]
    }
   ],
   "source": [
    "!docker push {IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can push the job to AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f'dna_methylation_xgboost_binary_{date}'\n",
    "MODEL_DIR= f\"gs://$BUCKET_NAME/job_dir/{date}\"\n",
    "REGION = \"us-east1\"\n",
    "HPTUNING_CONFIG = 'ai-platform-training/trainer/hptuning_config_xgboost.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [dna_methylation_xgboost_binary_20201021_212655] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe dna_methylation_xgboost_binary_20201021_212655\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs dna_methylation_xgboost_binary_20201021_212655\n",
      "jobId: dna_methylation_xgboost_binary_20201021_212655\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} --region {REGION} --master-image-uri {IMAGE_URI} --config {HPTUNING_CONFIG} -- --model-dir={MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Platform Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing my_custom_code.egg-info/PKG-INFO\n",
      "writing dependency_links to my_custom_code.egg-info/dependency_links.txt\n",
      "writing top-level names to my_custom_code.egg-info/top_level.txt\n",
      "reading manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "writing manifest file 'my_custom_code.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating my_custom_code-0.2\n",
      "creating my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying files to my_custom_code-0.2...\n",
      "copying predictor.py -> my_custom_code-0.2\n",
      "copying setup.py -> my_custom_code-0.2\n",
      "copying my_custom_code.egg-info/PKG-INFO -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/SOURCES.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/dependency_links.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "copying my_custom_code.egg-info/top_level.txt -> my_custom_code-0.2/my_custom_code.egg-info\n",
      "Writing my_custom_code-0.2/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'my_custom_code-0.2' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd ai-platform-prediction/prediction_routing/ && python setup.py sdist --format=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dna_cancer_prediction'\n",
    "VERSION_NAME='xgboost_binary_test'\n",
    "ORIGIN='gs://dna-methylation-cancer/job_dir/20201021_204518/'\n",
    "PACKAGE_URIS='gs://dna-methylation-cancer/prediction_routine/my_custom_code-0.2.tar.gz'\n",
    "PREDICTION_CLASS='predictor.MyPredictor'\n",
    "RUNTIME_VERSION='2.2'\n",
    "PYTHON_VERSION='3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://ai-platform-prediction/prediction_routing/dist/my_custom_code-0.2.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ai-platform-prediction/prediction_routing/dist/my_custom_code-0.2.tar.gz {PACKAGE_URIS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/gcp-nyc/models/dna_cancer_prediction].\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} --regions 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create {VERSION_NAME} --model {MODEL_NAME} --runtime-version {RUNTIME_VERSION} --python-version {PYTHON_VERSION} --origin {ORIGIN} --package-uris {PACKAGE_URIS} --prediction-class {PREDICTION_CLASS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
