{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Cancer with DNA Methylation\n",
    "\n",
    "This notebook is a walk-through of our project submitted to the 2020 Hack for Social Good\n",
    " _Build Hackathon.\n",
    "\n",
    "This notebook is organized the following way:\n",
    " 1. Data preparation\n",
    " 2. Preprocessing\n",
    " 3. Model Training and Evaluation\n",
    " 4. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment setup\n",
    "This notebook has been tested with Python 3.8.5. If not done already, create the `DNA_Methylation`\n",
    "virtual environment using conda and the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.protobuf.descriptor' has no attribute '_internal_create_key'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-c3239056adb7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mfeature_engineering\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpreprocessing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbigquery\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauth\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[0m__version__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_distribution\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"google-cloud-bigquery\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mversion\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclient\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mClient\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAccessEntry\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_helpers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_del_sub_prop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_http\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mConnection\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pandas_helpers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     62\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDatasetListItem\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m     \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbigquery_storage_v1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pragma: NO COVER\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[0mbigquery_storage_v1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery_storage_v1\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m ).version  # noqa\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtypes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mclient\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgapic\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0menums\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery_storage_v1\\types.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapi_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf_helpers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mget_messages\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproto\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0marrow_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproto\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mavro_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcloud\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbigquery_storage_v1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mproto\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstorage_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Miniconda3\\envs\\DNA_Methylation\\lib\\site-packages\\google\\cloud\\bigquery_storage_v1\\proto\\arrow_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0msyntax\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"proto3\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mserialized_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mb\"\\n$com.google.cloud.bigquery.storage.v1B\\nArrowProtoP\\001ZGgoogle.golang.org/genproto/googleapis/cloud/bigquery/storage/v1;storage\\252\\002 Google.Cloud.BigQuery.Storage.V1\\312\\002 Google\\\\Cloud\\\\BigQuery\\\\Storage\\\\V1\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[0mcreate_key\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_descriptor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_internal_create_key\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m     \u001B[0mserialized_pb\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mb'\\n2google/cloud/bigquery_storage_v1/proto/arrow.proto\\x12 google.cloud.bigquery.storage.v1\"(\\n\\x0b\\x41rrowSchema\\x12\\x19\\n\\x11serialized_schema\\x18\\x01 \\x01(\\x0c\"F\\n\\x10\\x41rrowRecordBatch\\x12\\x1f\\n\\x17serialized_record_batch\\x18\\x01 \\x01(\\x0c\\x12\\x11\\n\\trow_count\\x18\\x02 \\x01(\\x03\\x42\\xc3\\x01\\n$com.google.cloud.bigquery.storage.v1B\\nArrowProtoP\\x01ZGgoogle.golang.org/genproto/googleapis/cloud/bigquery/storage/v1;storage\\xaa\\x02 Google.Cloud.BigQuery.Storage.V1\\xca\\x02 Google\\\\Cloud\\\\BigQuery\\\\Storage\\\\V1b\\x06proto3'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'google.protobuf.descriptor' has no attribute '_internal_create_key'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, \\\n",
    "                           plot_confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from feature_engineering.get_data import read_dataset, read_from_gcs, \\\n",
    "    download_data, configure_gcs\n",
    "from feature_engineering.preprocessing import preprocessing\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "import google.auth\n",
    "\n",
    "from configs import configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We tested this notebook with:\n",
    " - Pandas version used: 1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version used: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pandas version used: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Retrieve credentials to use GCP's Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcp-nyc\n"
     ]
    }
   ],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = configs.PROJECT_ID\n",
    "DATASET = configs.DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "In the data preparation part, we will create two bigquery tables that will used to create the\n",
    "final train and test datasets.\n",
    "\n",
    "The first BigQuery table is called `tcga_betas` is a BigQuery partitioned and clustered table\n",
    "that will hold all betas values for all TCGA patients. It will be a table in a long format:\n",
    "one row per betas observation.\n",
    "\n",
    "The second BigQuery table is called `columns_to_keep` is a BigQuery table that will hold the\n",
    "list of CpG site that we will keep in the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First table - `tcga_betas` table: a simple SQL script\n",
    "\n",
    "This first table will be created using a simple SQL query. The query is located in\n",
    "`SQL_queries/tcga_betas.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE dna_cancer_prediction.tcga_betas\n",
      " PARTITION BY RANGE_BUCKET(row_number, GENERATE_ARRAY(1, 11000, 100))\n",
      " CLUSTER BY row_number\n",
      " AS\n",
      " \n",
      " WITH\n",
      "   brca_betas AS (\n",
      "   SELECT\n",
      "     MIM.CpG_probe_id,\n",
      "     dna.case_barcode,\n",
      "     dna.aliquot_barcode,\n",
      "     dna.probe_id,\n",
      "     dna.beta_value,\n",
      "     gc.Name\n",
      "   FROM\n",
      "     `isb-cgc.platform_reference.methylation_annotation` gc\n",
      "   JOIN\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation` dna\n",
      "   ON\n",
      "     gc.Name = dna.probe_id\n",
      "   JOIN\n",
      "     `isb-cgc.platform_reference.GDC_hg38_methylation_annotation` MIM\n",
      "   ON\n",
      "     MIM.CpG_probe_id = gc.Name),\n",
      " \n",
      "   participants_id AS (\n",
      "   SELECT\n",
      "     DISTINCT case_barcode\n",
      "   FROM\n",
      "     `isb-cgc.TCGA_hg38_data_v0.DNA_Methylation`),\n",
      " \n",
      "   participants_id_numbered AS (\n",
      "   SELECT\n",
      "     case_barcode,\n",
      "     ROW_NUMBER() OVER (order by case_barcode) as row_number\n",
      "   FROM\n",
      "     participants_id)\n",
      " \n",
      " SELECT\n",
      "   A.case_barcode,\n",
      "   A.beta_value,\n",
      "   A.CpG_probe_id,\n",
      "   A.aliquot_barcode,\n",
      "   SUBSTR(A.aliquot_barcode, 14, 2) AS sample_id,\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('10', '11', '12', '13', '14', '15', '16', '17', '18', '19'), \"normal\",\n",
      "   IF\n",
      "   (SUBSTR(A.aliquot_barcode, 14, 2) IN ('01', '02', '03', '04',  '05', '06', '07', '08', '09'), 'tumor', \"control\")) AS sample_status,\n",
      "   B.row_number\n",
      " FROM\n",
      "   brca_betas A\n",
      " LEFT JOIN\n",
      "   participants_id_numbered B\n",
      " ON A.case_barcode = B.case_barcode\n"
     ]
    }
   ],
   "source": [
    "SQL_query_path = 'sql_queries/tcga_betas.txt'\n",
    "with open(SQL_query_path, 'r') as f:\n",
    "    sql_query = ' '.join(f.readlines())\n",
    "\n",
    "sql_query = sql_query.replace('__DATASET__', DATASET)\n",
    "sql_query = sql_query.replace('__TABLE_NAME__', 'tcga_betas')\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TO DO: describe query\n",
    "\n",
    "We can execute the job to create the table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "query_job = client.query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Second table - `columns_to_keep`: using dataproc\n",
    "\n",
    "The second table will be created by a Dataflow job. This table will hold the list of CpG sites\n",
    "that will be included in the training dataset.\n",
    "\n",
    "Indeed, we want to keep only the 5,000 best CpG site among the 500,000 that will be able to\n",
    "separate cancerous vs non-cancerous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [81fa880ca37c4901bc736ce3e941c5ea] submitted.\n",
      "Waiting for job output...\n",
      "build_hackathon_dnanyc_tfx_pipeline\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.util.log: Logging initialized @2728ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.Server: Started @2816ms\n",
      "20/10/20 14:20:36 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:20:36 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at build-hackathon-nyc-cluster-m/10.0.0.39:8032\n",
      "20/10/20 14:20:37 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at build-hackathon-nyc-cluster-m/10.0.0.39:10200\n",
      "20/10/20 14:20:39 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1603203106846_0002\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[aliquot_barcode], filters=[]\n",
      "20/10/20 14:20:46 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[aliquot_barcode], filter=''\n",
      "20/10/20 14:20:50 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGRFamNsRlVxTHJXNBoCaXIaAmpk\n",
      "20/10/20 14:25:00 WARN org.apache.spark.sql.Column: Constructing trivially true equals predicate, 'CpG_probe_id#2 = CpG_probe_id#2'. Perhaps you need to use aliases.\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id,sample_status], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:01 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id, sample_status], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEtCZlRZTWF4c0I3VxoCaXIaAmpk\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[CpG_probe_id], filters=[IsNotNull(CpG_probe_id)]\n",
      "20/10/20 14:25:03 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[CpG_probe_id], filter='(`CpG_probe_id` IS NOT NULL)'\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDGxYLTZ2RWlsaEc4YhoCaXIaAmpk\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table gcp-nyc.dna_cancer_prediction.tcga_betas, parameters sent from Spark: requiredColumns=[beta_value,CpG_probe_id], filters=[]\n",
      "20/10/20 14:25:05 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from gcp-nyc.dna_cancer_prediction.tcga_betas columns=[beta_value, CpG_probe_id], filter=''\n",
      "20/10/20 14:25:07 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Created read session for table 'gcp-nyc.dna_cancer_prediction.tcga_betas': projects/gcp-nyc/locations/us/sessions/CAISDEwwSkt5V2hqQi1lcBoCaXIaAmpk\n",
      "20/10/20 14:35:11 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Submitted load to GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=dna_cancer_prediction, projectId=gcp-nyc, tableId=cpg_site_list}}. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO com.google.cloud.spark.bigquery.BigQueryWriteHelper: Done loading to gcp-nyc.dna_cancer_prediction.cpg_site_list. jobId: JobId{project=gcp-nyc, job=38ef4737-d3ae-4f3c-829d-ec5d0c8f0b3f, location=US}\n",
      "20/10/20 14:35:18 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@e5b4927{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.deploy.yarn.Client: Failed to contact YARN for application application_1603203106846_0002.\n",
      "java.io.InterruptedIOException: Call interrupted\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1504)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1456)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1366)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:232)\n",
      "\tat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy16.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:496)\n",
      "\tat org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:301)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1060)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:109)\n",
      "20/10/20 14:35:18 ERROR org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend: Yarn application has already exited with state FAILED!\n",
      "Job [81fa880ca37c4901bc736ce3e941c5ea] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/\n",
      "driverOutputResourceUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/driveroutput\n",
      "jobUuid: 96de5e5c-9a94-3efb-98d4-15dc226da94f\n",
      "placement:\n",
      "  clusterName: build-hackathon-nyc-cluster\n",
      "  clusterUuid: 11f36b0a-7180-44e2-8f22-a219c38293fb\n",
      "pysparkJob:\n",
      "  args:\n",
      "  - -gcs_bucket\n",
      "  - build_hackathon_dnanyc_tfx_pipeline\n",
      "  jarFileUris:\n",
      "  - gs://spark-lib/bigquery/spark-bigquery-latest.jar\n",
      "  mainPythonFileUri: gs://build_hackathon_dnanyc/google-cloud-dataproc-metainfo/11f36b0a-7180-44e2-8f22-a219c38293fb/jobs/81fa880ca37c4901bc736ce3e941c5ea/staging/create_cpg_sites_list.py\n",
      "reference:\n",
      "  jobId: 81fa880ca37c4901bc736ce3e941c5ea\n",
      "  projectId: gcp-nyc\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2020-10-20T14:35:23.713Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2020-10-20T14:20:31.861Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2020-10-20T14:20:31.926Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2020-10-20T14:20:32.149Z'\n",
      "yarnApplications:\n",
      "- name: my_app\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://build-hackathon-nyc-cluster-m:8088/proxy/application_1603203106846_0002/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster build-hackathon-nyc-cluster \\\n",
    "    --region us-east1 --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar \\\n",
    "    dataproc_processing/create_cpg_sites_list.py \\\n",
    "    -- -gcs_bucket \"build_hackathon_dnanyc_tfx_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating training dataset\n",
    "\n",
    "We will now use those training tables to create our training dataset. The training dataset will be\n",
    "saved into a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = configs.DATASET_PATH\n",
    "DATASET_NAME = 'tcga-binary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def configure_gcs(project_id=PROJECT_ID):\n",
    "    client = storage.Client(project=project_id)\n",
    "    return client\n",
    "\n",
    "def save_to_gcs(df, gcs_path, file_name):\n",
    "    df.to_csv(gcs_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_bigquery(project_id, dataset, table, list_of_columns, start_index, end_index):\n",
    "    formatted_columns = \"', '\".join(list_of_columns)\n",
    "    query = f\"\"\"\n",
    "    SELECT beta_value, CpG_probe_id, aliquot_barcode, sample_status\n",
    "    from `{dataset}.{table}`\n",
    "    where CpG_probe_id in ('{formatted_columns}') and row_number >= {start_index} and row_number <= {end_index}\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df\n",
    "\n",
    "def download_columns_to_keep(project_id, dataset, table):\n",
    "    query = f\"\"\"\n",
    "    SELECT CpG_probe_id\n",
    "    from `{dataset}.{table}`\n",
    "    \"\"\"\n",
    "    df = pd.read_gbq(query, project_id=project_id)\n",
    "    return df['CpG_probe_id'].values\n",
    "\n",
    "def merge_and_pivot_by_slices(project_id, dataset, table, columns, start_index, end_index):\n",
    "    df_betas = download_from_bigquery(project_id, dataset, table, columns, start_index, end_index)\n",
    "    df_p = df_betas.pivot(index=\"aliquot_barcode\", columns='CpG_probe_id', values='beta_value').reset_index()\n",
    "    df_labels = df_betas[['aliquot_barcode', 'sample_status']].drop_duplicates()\n",
    "    df_final = df_p.merge(df_labels,\n",
    "                          how='left', on='aliquot_barcode')\n",
    "    df_final = df_final.set_index('aliquot_barcode')\n",
    "    # Reorder columns\n",
    "    df_final = df_final[columns]\n",
    "    return df_final\n",
    "\n",
    "def pivot_data(project_id, dataset, betas_table, cpg_site_list_table, max_index=11000, steps=1000):\n",
    "    columns = download_columns_to_keep(project_id, dataset, cpg_site_list_table)\n",
    "    df = merge_and_pivot_by_slices(project_id, dataset, betas_table, columns, 1, steps)\n",
    "    start_index = steps + 1\n",
    "    for i in range(start_index, max_index, steps):\n",
    "        print(f\"Processing index {i} to {i + steps}\")\n",
    "        new_df = merge_and_pivot_by_slices(project_id, dataset, betas_table, columns, i, i + steps)\n",
    "        df = df.append(new_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gcs_client = configure_gcs(PROJECT_ID)\n",
    "columns = download_columns_to_keep(PROJECT_ID, DATASET, 'cpg_site_list')\n",
    "df = pivot_data(PROJECT_ID, DATASET, 'tcga_betas', 'cpg_site_list')\n",
    "save_to_gcs(df, DATASET_PATH, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Training\n",
    "\n",
    "Using the saved dataset in Google Cloud Storage, we will now preprocess the dataset to create\n",
    "a train and test split, train our model on the training set and output and evaluation metric\n",
    "on the evaluation set.\n",
    "\n",
    "We chose to an XGBoost model as part of this project. It's a model that first gives a very good\n",
    "accuracy on complex problems. The model is also easy to interpret as it is a tree based model."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data from Google Cloud Storage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-bbc0adea1513>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mBUCKET_NAME\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfigs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGCS_BUCKET\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mRAW_DATASET_PATH\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfigs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRAW_DATASET_PATH\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# TO DO: update label and index names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mLABELS\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'sample_status'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mINDEX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'aliquot_barcode'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'configs' is not defined"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = configs.GCS_BUCKET\n",
    "RAW_DATASET_PATH = configs.RAW_DATASET_PATH\n",
    "# TO DO: update label and index names\n",
    "LABELS = 'sample_status'\n",
    "INDEX = 'aliquot_barcode'\n",
    "betas, labels, cpg_sites, index = read_from_gcs(BUCKET_NAME, RAW_DATASET_PATH, INDEX, LABELS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Map classes to a binary integer classification. 1 will represent a tumorous observation and 0 will represent a normal\n",
    "observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "map_labels_to_classes = {\n",
    "  'tumor': 1,\n",
    "  'normal': 0\n",
    "}\n",
    "labels = [map_labels_to_classes[elt] for elt in labels]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-balance dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, labels, cpg_sites = preprocessing(betas, labels, cpg_sites, smote=True,\n",
    "                                                                   fill_na_strategy='knn',\n",
    "                                                                    sampling_strategy=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_train))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(bargroupgap=0.2,\n",
    "                  title=\"Histogram of training labels after rebalancing the dataset\",\n",
    "                  xaxis_title=\"Cancerous or normal observations\",\n",
    "                  yaxis_title=\"Count of observations\")\n",
    "fig.update_layout(\n",
    "    margin=dict(\n",
    "        l=40,\n",
    "        r=40,\n",
    "        b=40,\n",
    "        t=40,\n",
    "        pad=4\n",
    "    ),\n",
    "    title={\n",
    "        'x':0.5})\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "Now that we have chosen XGBoost, we need to find the hyper-parameters that will provide the best accuracy on\n",
    "our dataset.\n",
    "To do this, we are defining a Stratified-KFold crossvalidation. Stratified means that we are keeping the same\n",
    "proportion of positive and negative observations in each validation fold.\n",
    "Here, we choose to create 4 folds. This means that we will do four fits: each time, training on 3/4 of the training\n",
    "data while testing on the remaining third.\n",
    "After all these fits, we average the F1 score we got and take the hyper-parameter combination that gives the best\n",
    "average F1 score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explain the XGBoost parameters we are using:\n",
    " - `objective=binary:logistic`: this is a binary classification problem, and we want to ouput the probability that\n",
    "   an observation is cancerous\n",
    " - `colsample_bytree=0.8`: we are considering only 80% of features to build each tree. This prevents over-fitting\n",
    " - `learning_rate=0.1`: prevents over-fitting by shrinking the feature weights\n",
    " - `subsample=0.8`: we are considering only 80% of the observations to build each tree. This prevents over-fitting.\n",
    " - `nthread=4`: uses 4 different thread to train the model. Enables parallel processing.\n",
    "\n",
    "In our parameter grid, we are testing the following parameters:\n",
    " - `max_depth`: the maximum depth of each tree in the model. The higher the parameter, the higher the variance\n",
    " - `min_child_weight`: the minimum sum of hessian weight needed to create a child node and partition further the tree.\n",
    "   The higher the parameter, the lower the variance.\n",
    " - `gamma`: the minimum loss reduction required to make a further partition on a leaf node. The higher the parameter,\n",
    "   the lower the variance.\n",
    " - `alpha`: L1 regularisation term on weights. The higher the parameter, the lower the variance.\n",
    " - `n_estimator`: number of trees in the model. The higher the parameter, the higher the variance.\n",
    "\n",
    "See [here](https://xgboost.readthedocs.io/en/latest/parameter.html) for the complete XGBoost documentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': range(3,10,2),\n",
    "    'min_child_weight': range(1,6,2),\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'alpha':[6,8,10,12],\n",
    "    'n_estimators': [1e2, 1e3, 1e4]\n",
    "}\n",
    "estimator = xgb.XGBClassifier(objective= 'binary:logistic',\n",
    "                              colsample_bytree = 0.8,\n",
    "                              learning_rate = 0.1,\n",
    "                              subsample=0.8,\n",
    "                              nthread=4)\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid = param_test1,\n",
    "                        scoring='f1_weighted',n_jobs=4, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's retrieve the best parameters from the GridSearch optimizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_max_depth = best_params['max_depth']\n",
    "best_min_child_weight = best_params['min_child_weight']\n",
    "best_gamma = best_params['gamma']\n",
    "best_alpha = best_params['alpha']\n",
    "best_n_estimators = best_params['n_estimators']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can fit the model with the optimized hyper-parameters on the entire dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective ='multi:softmax',\n",
    "                           colsample_bytree = 0.8,\n",
    "                           learning_rate = 0.1,\n",
    "                           subsample=0.8,\n",
    "                           nthread=4,\n",
    "                           max_depth=best_max_depth,\n",
    "                           min_child_weight=best_min_child_weight,\n",
    "                           gamma=best_gamma,\n",
    "                           alpha=best_alpha,\n",
    "                           n_estimators=best_n_estimators)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's now analyze the model performance\n",
    "\n",
    "### F1 Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compute the f1 score as a rough proxy for model performance. First, on the training set and then on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "training_predicted_labels = model.predict(X_train)\n",
    "f1_training = f1_score(training_predicted_labels, y_train)\n",
    "print(f\"The train accuracy is {f1_training:.3f}\")\n",
    "\n",
    "# Testing\n",
    "testing_predicted_labels = model.predict(X_test)\n",
    "f1_testing = f1_score(testing_predicted_labels, y_test)\n",
    "print(f\"The test accuracy is {f1_testing:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "To go one level deeper, we can output the confusion matrix on both the training set and the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the *training* set\")\n",
    "plot_confusion_matrix(model, X_train, y_train, ax=ax, values_format='.0f', normalize=None);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "f.suptitle(\"Confusion matrix on the test set\")\n",
    "plot_confusion_matrix(model, X_test, y_test, ax=ax, values_format='.0f', normalize=None)\n",
    "f.tight_layout()\n",
    "f.savefig(\"confusion_matrix.png\", dpi=200)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}